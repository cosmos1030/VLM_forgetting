{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/clifford-data/home/doyoonkim/miniconda3/envs/doyoon/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-24 08:37:32.518718: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-24 08:37:32.541583: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750754252.566407 1348225 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750754252.574506 1348225 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750754252.594838 1348225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750754252.594865 1348225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750754252.594867 1348225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750754252.594869 1348225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-24 08:37:32.604169: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n",
      "/clifford-data/home/doyoonkim/miniconda3/envs/doyoon/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2VisionTransformerPretrainedModel(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "  )\n",
       "  (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x Qwen2VLVisionBlock(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): VisionSdpaAttention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp): VisionMlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): QuickGELUActivation()\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (merger): PatchMerger(\n",
       "    (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, Qwen2VLForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-2B\"\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "\n",
    "\n",
    "# 비전 인코더만 가져오기\n",
    "vision_encoder = model.model.visual\n",
    "# vision_encoder = torch.nn.DataParallel(model.model.visual).eval().cuda()\n",
    "\n",
    "vision_encoder.eval().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(64,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# 예시 이미지 불러오기\n",
    "img = Image.open(requests.get(\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\", stream=True).raw)\n",
    "\n",
    "# 이미지 전처리 (output: pixel_values, grid_thw)\n",
    "processed = processor.image_processor(images=img, return_tensors=\"pt\")\n",
    "pixel_values = processed[\"pixel_values\"].cuda()        # shape: (1, 3, D, H, W)\n",
    "grid_thw     = processed[\"image_grid_thw\"].cuda()      # shape: (1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14308, 1176])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_thw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = vision_encoder(pixel_values, grid_thw)  # shape: (tokens, dim=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = vision_encoder(pixel_values, grid_thw)  # shape: (tokens, dim=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3577, 1536])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_encoder.config.hidden_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLVisionConfig {\n",
       "  \"depth\": 32,\n",
       "  \"embed_dim\": 1280,\n",
       "  \"hidden_act\": \"quick_gelu\",\n",
       "  \"hidden_size\": 1536,\n",
       "  \"in_channels\": 3,\n",
       "  \"in_chans\": 3,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"mlp_ratio\": 4,\n",
       "  \"model_type\": \"qwen2_vl\",\n",
       "  \"num_heads\": 16,\n",
       "  \"patch_size\": 14,\n",
       "  \"spatial_merge_size\": 2,\n",
       "  \"spatial_patch_size\": 14,\n",
       "  \"temporal_patch_size\": 2,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.52.3\"\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_encoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../vlm_forgetting\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attentive_pooler import AttentivePooler, AttentiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3577, 1536])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentive_pooler = AttentiveClassifier(\n",
    "        embed_dim=vision_encoder.config.hidden_size,\n",
    "        num_heads=vision_encoder.config.num_heads,\n",
    "        depth=1,\n",
    "        num_classes=10,\n",
    "\n",
    "    ).cuda()\n",
    "out = out.cuda()\n",
    "out = out.unsqueeze(0)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/clifford-data/home/doyoonkim/miniconda3/envs/doyoon/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0374, -0.2016, -0.1618, -0.0218,  0.3811,  0.3574,  0.0160,  0.4745,\n",
       "          0.2671,  0.2352]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentive_pooler(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doyoon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
